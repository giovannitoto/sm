---
title: "03_Pulizia_Tweet"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidytext)
require(dplyr)
library(TextWiller)
library(magrittr)    # per rimuovere piu' parole insieme
library(rtweet)      # ts_plot
library(lubridate)   # as_datetime
rm(list=ls())

# Importo il dataset ed elimino le varibili non importanti
load("Tweets.RData")
tw <- tweets[tweets$language=="it",]
rm(tweets)
```

## Pulizia dei tweets
Prendiamo i tweet in lingua italiana, mantenendo le informazioni del partito e dell'username, a questo punto utilizziamo la funzione normalizzaTesti che redne confrontabili i tweet normalizzando gli url, i caratteri speciali, le emoticons, la punteggiatura ecc...

```{r}
for(i in unique(tw$partito)){
  cat(i, "\n")
  for(j in unique(tw$username[tw$partito == i])){
    cat(j, ", ")
    tt <- tw[tw$username == j,]
    # Tengo solo testo del tweet, username, partito e creo indice di riga
    tt <- tt %>% select(tweet, username=username, partito=partito) %>% mutate(id = 1:n())
    
    # Normalizzo i tweet facendo cose
    tt$tweet <- normalizzaTesti(tt$tweet,
                      tolower = TRUE,
                      normalizzahtml = TRUE,
                      normalizzacaratteri = TRUE,
                      normalizzaemote = TRUE,
                      normalizzaEmoticons = TRUE,
                      normalizzapunteggiatura = TRUE,
                      normalizzaslang = TRUE,
                      fixed = TRUE,
                      perl = TRUE,
                      preprocessingEncoding = TRUE,
                      encoding = "UTF-8",
                      sub = "",
                      contaStringhe = c("\\?", "\\!", "@", "#", "(\200|euro)", "(\\$|dollar)",
                                        "SUPPRESSEDTEXT"),
                      suppressInvalidTexts = TRUE,
                      verbatim = TRUE,
                      remove = TRUE,
                      removeUnderscore = FALSE)
    tt$tweet <- str_remove(tt$tweet, "wwwurlwww")
    tw$tweet[tw$username == j] <- tt$tweet
  }
  cat("\n")
}
```

Rimuoviamo le stopword all'interno della stop list "stopword-it.txt":
```{r}
stop_words <- read.table("stopwords-it.txt", encoding="UTF-8",
                         quote="\"", comment.char="")$V1
stopwords_regex <- paste(stop_words, collapse = '\\b|\\b')
stopwords_regex <- paste0('\\b', stopwords_regex, '\\b')

for(i in unique(tw$partito)){
  cat(i, "\n")
  for(j in unique(tw$username[tw$partito == i])){
    cat(j, ", ")
    tw$tweet[tw$username == j] <- stringr::str_replace_all(tw$tweet[tw$username == j], stopwords_regex, "")
  }
  cat("\n")
}

```
```{r}
save(tw, file = "Tweets_pulito.RData")
```

## Analisi esplorative
### Monogrammi

```{r}
tw %>% unnest_tokens(word,tweet) %>% 
  group_by(word) %>% summarise(tot=n()) %>% 
  arrange(desc(tot)) %>% print(n=80)
```


### Bigrammi

```{r}

ttU = tt %>% unnest_ngrams(word, tweet, n=2)
#parole che vogliamo eliminare
tt$tweet = str_remove(tt$tweet, "non | oggi |piÃ¹")
tt$tweet = str_remove(tt$tweet, "aggiornamento \\d+")
#aggiornamento1, 2, ..d+ sta per numeri
tt$tweet = gsub("\\b[0-9\\W]+\\b", "", tt$tweet)#gsub 
#rimuovo parole unicamente composti da cifre
tt$tweet = wordStem(tt$tweet, language = "it")
ttU = tt %>% unnest_ngrams(word, tweet, n=2) #bigrammi

tt %>% unnest_ngrams(word,tweet,n=2) %>% group_by(word) %>% #raggruppo per parola
  summarise(tot=n()) %>% 
  arrange(desc(tot)) %>% print(n=20)
```


