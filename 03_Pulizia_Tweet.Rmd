---
title: "03_Pulizia_Tweet"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidytext)
require(dplyr)
library(TextWiller)
library(magrittr)    # per rimuovere piu' parole insieme
library(rtweet)      # ts_plot
library(lubridate)   # as_datetime
rm(list=ls())

# Importo il dataset ed elimino le varibili non importanti
load("Tweets.RData")
tw <- tweets[tweets$language=="it",]
rm(tweets)
```

## Pulizia dei tweets
Prendiamo i tweet in lingua italiana, mantenendo le informazioni del partito e dell'username, a questo punto utilizziamo la funzione normalizzaTesti che redne confrontabili i tweet normalizzando gli url, i caratteri speciali, le emoticons, la punteggiatura ecc...

```{r}
for(i in unique(tw$partito)){
  cat(i, "\n")
  for(j in unique(tw$username[tw$partito == i])){
    cat(j, ", ")
    tt <- tw[tw$username == j,]
    # Tengo solo testo del tweet, username, partito e creo indice di riga
    tt <- tt %>% select(tweet, username=username, partito=partito) %>% mutate(id = 1:n())
    
    # Normalizzo i tweet facendo cose
    tt$tweet <- normalizzaTesti(tt$tweet,
                      tolower = TRUE,
                      normalizzahtml = TRUE,
                      normalizzacaratteri = TRUE,
                      normalizzaemote = TRUE,
                      normalizzaEmoticons = TRUE,
                      normalizzapunteggiatura = TRUE,
                      normalizzaslang = TRUE,
                      fixed = TRUE,
                      perl = TRUE,
                      preprocessingEncoding = TRUE,
                      encoding = "UTF-8",
                      sub = "",
                      contaStringhe = c("\\?", "\\!", "@", "#", "(\200|euro)", "(\\$|dollar)",
                                        "SUPPRESSEDTEXT"),
                      suppressInvalidTexts = TRUE,
                      verbatim = TRUE,
                      remove = TRUE,
                      removeUnderscore = FALSE)
    tt$tweet <- str_remove(tt$tweet, "wwwurlwww")
    tw$tweet[tw$username == j] <- tt$tweet
  }
  cat("\n")
}
```

Rimuoviamo le stopword all'interno della stop list "stopword-it.txt":
```{r}
stop_words <- read.table("stopwords-it.txt", encoding="UTF-8",
                         quote="\"", comment.char="")$V1
stopwords_regex <- paste(stop_words, collapse = '\\b|\\b')
stopwords_regex <- paste0('\\b', stopwords_regex, '\\b')

for(i in unique(tw$partito)){
  cat(i, "\n")
  for(j in unique(tw$username[tw$partito == i])){
    cat(j, ", ")
    tw$tweet[tw$username == j] <- stringr::str_replace_all(tw$tweet[tw$username == j], stopwords_regex, "")
  }
  cat("\n")
}

```
Togliamo le parole che sono usate con meno frequenza:
```{r}
parole_poco_usate <- tw %>% mutate(id = 1:n()) %>% unnest_tokens(word,tweet) %>% group_by(id) %>% group_by(word)  


parole_poco_usate <- parole_poco_usate %>% 
                      summarise(tot=n(), freq = n()/nrow(parole_poco_usate)) %>% 
                        arrange(tot) 

sum(parole_poco_usate$tot)

nrow(parole_poco_usate[parole_poco_usate$freq < 0.01,])
```


Togliamo delle sequenze particolari di caratteri:
```{r}
tw$tweet = str_remove(tw$tweet, "aggiornamento \\d+")
#aggiornamento1, 2, ..d+ sta per numeri
tw$tweet = gsub("\\b[0-9\\W]+\\b", "", tw$tweet)#gsub 
#rimuovo parole unicamente composti da cifre

# rimuovo le emoticon
# tw$tweet = gsub("emote_\\w+:\\w+", "", tw$tweet)
tw$tweet = gsub("emote_\\w+", "", tw$tweet)
```

Riscriviamo le emoticon delle bandiere perchÃ¨ sono viste come due parole "emoticon_flag:_stato":
```{r}
x <- unique(str_extract(tw$tweet, "emote_\\w+:\\w+"))[-1]
x <- x[order(nchar(x), x, decreasing = T)]
y <- sapply(x, function(k) str_replace(k, ":", "_"))

for(i in 1:length(x)) str_replace(tw$tweet, x[i], y[i])
```





```{r}
#save(tw, file = "Tweets_pulito.RData")
```
